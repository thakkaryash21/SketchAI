{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83885d1-36a4-45bb-a0ad-81d1763a62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch pillow\n",
    "# !pip install opencv-python-headless\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8a89dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union, List, Tuple\n",
    "from PIL import Image\n",
    "import torch\n",
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06ca846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/sagemaker-user/img2text'\n",
    "config = {\n",
    "    \"sketches\": f\"{root}/sketches\",\n",
    "    \"ground_truth\": f\"{root}/ground_truth.json\",\n",
    "    \"epochs\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5b545-76da-47bf-a4ca-50461663ce14",
   "metadata": {},
   "source": [
    "# Load Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a6125b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading bike: 100%|██████████| 9/9 [00:00<00:00, 5534.19it/s]\n",
      "Loading car: 100%|██████████| 10/10 [00:00<00:00, 8184.01it/s]\n",
      "Loading cat: 100%|██████████| 7/7 [00:00<00:00, 7580.72it/s]\n",
      "Loading cycle: 100%|██████████| 6/6 [00:00<00:00, 4531.12it/s]\n",
      "Loading plane: 100%|██████████| 10/10 [00:00<00:00, 6151.81it/s]\n",
      "Loading signal: 100%|██████████| 6/6 [00:00<00:00, 3076.51it/s]\n"
     ]
    }
   ],
   "source": [
    "metadata = json.load(open(f'{config[\"sketches\"]}/metadata.json'))\n",
    "\n",
    "sketches = []\n",
    "\n",
    "for key in metadata:\n",
    "    for i in tqdm(range(metadata[key]),desc= f'Loading {key}'):\n",
    "        file_name = key+ f\"_{i+1}\"\n",
    "        sketch = Image.open(os.path.join( config['sketches'], file_name+\".png\"))\n",
    "        sketches.append((file_name, sketch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d2a7a-0b6a-4045-a247-70199538300e",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2429eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64b386-04c2-45b3-b831-351d3aa79fa6",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1db1301-9343-4771-a09d-ce03d77090e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Caption...:   0%|          | 0/48 [00:00<?, ?it/s]/home/sagemaker-user/.conda/envs/stable-diffusion/lib/python3.9/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Generating Caption...: 100%|██████████| 48/48 [01:02<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike_1': 'a motorcycle with a rider on it', 'bike_2': 'a motorcycle is shown in the shape of a motorcycle', 'bike_3': 'a drawing of a bicycle', 'bike_4': 'a drawing of a man with a gun', 'bike_5': 'a motorcycle with a side view', 'bike_6': 'a drawing of a motorcycle', 'bike_7': 'a drawing of a person riding a bike', 'bike_8': 'a motorcycle with a helmet and helmet on it', 'bike_9': 'a motorcycle is shown in the shape of a motorcycle', 'car_1': 'a drawing of a car', 'car_2': 'a map of the state of new york', 'car_3': 'a car with a white background', 'car_4': 'a car with the number plate removed', 'car_5': 'a car is shown in the shape of a car', 'car_6': 'a drawing of a truck', 'car_7': 'a car is shown in the shape of a car', 'car_8': 'a drawing of a truck', 'car_9': 'a car with wheels and wheels', 'car_10': 'a drawing of a car', 'cat_1': 'a black and white drawing of a cat', 'cat_2': 'a black and white drawing of a cat', 'cat_3': \"a drawing of a cat ' s face\", 'cat_4': 'a black and white drawing of a cat', 'cat_5': 'a drawing of a fish', 'cat_6': 'a black and white drawing of a cat', 'cat_7': \"a drawing of a cat ' s face\", 'cycle_1': 'a bicycle with a wheel on the front', 'cycle_2': 'a drawing of a bicycle', 'cycle_3': 'a bicycle with a propeller on the front wheel', 'cycle_4': 'a continuous drawing of a bicycle', 'cycle_5': 'a bicycle is shown in the shape of a bicycle', 'cycle_6': 'a bicycle with a bicycle wheel on it', 'plane_1': 'the logo for the new yorks', 'plane_2': 'a drawing of a shoe', 'plane_3': 'a line drawing of a plane', 'plane_4': 'a plane is shown in the shape of a plane', 'plane_5': 'a line drawing of a plane', 'plane_6': 'a black and white drawing of a person laying down', 'plane_7': 'a line drawing of a plane', 'plane_8': 'a plane with a nose drawn on it', 'plane_9': 'a small plane with a propeller and a landing wheel', 'plane_10': 'a drawing of a gun', 'signal_1': 'a line drawing of a person standing on a pole', 'signal_2': 'a traffic light with three lights on it', 'signal_3': 'a traffic light with a white background', 'signal_4': 'a line drawing of a person standing on a pole', 'signal_5': 'a drawing of a person holding a sign', 'signal_6': 'a line drawing of a key'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "for category, sketch in tqdm(sketches, desc=f\"Generating Caption...\"):\n",
    "    inputs = processor(sketch, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    result[category] = caption\n",
    "\n",
    "print(result)\n",
    "\n",
    "with open('img2text_baseline.json', 'w+') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacaf12-3d7d-4c5f-b76a-2f5f0f3795ed",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21808758-41ef-499c-a8f2-ddd9fb84a234",
   "metadata": {},
   "source": [
    "## Loading Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b313a8db-e077-4014-957d-2eb486dedbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike_1': 'Drawing of the side view of a motorcycle', 'bike_2': 'Isometric view drawing of a police motorcycle', 'bike_3': 'Isometric view drawing of a police motorcycle', 'bike_4': 'Isometric view drawing of a bike', 'bike_5': 'Isometric view drawing of a police motorcycle', 'bike_6': 'Isometric view drawing of a bullet motorcycle', 'bike_7': 'Front view drawing of a motorcycle', 'bike_8': 'Isometric view drawing of a motorcycle', 'bike_9': 'Drawing of a motorcycle', 'car_1': 'Sketch of a car', 'car_2': 'Isometric view drawing of a car', 'car_3': 'Drawing of a sedan car', 'car_4': 'Sketch of a race car with spoilers', 'car_5': 'Front view drawing of a car', 'car_6': 'Side view drawing of a car', 'car_7': 'Drawing of a car', 'car_8': 'Front view of a limousine car', 'car_9': 'Side view sketch of a car', 'car_10': 'Isometric view drawing of a classic car', 'cat_1': 'Sketch of a cat', 'cat_2': 'Outline Drawing of a cat', 'cat_3': \"Drawing of a cat's face\", 'cat_4': 'Side view sketch of a cat looking up', 'cat_5': 'Drawing of a cat', 'cat_6': 'Sketch of a cat', 'cat_7': 'Outline Drawing of a cat', 'cycle_1': 'Sketch of a bicycle', 'cycle_2': 'Sideview sketch of a bicycle', 'cycle_3': 'Sideview drawing of a bicycle', 'cycle_4': 'Sideview drawing of the front half of a bicycle', 'cycle_5': 'Drawing of a bicycle', 'cycle_6': 'Drawing of a bicycle', 'plane_1': 'Drawing of a plane', 'plane_2': 'Sideview drawing of a plane', 'plane_3': 'Sideview sketch of a plane', 'plane_4': 'Drawing of a plane flying', 'plane_5': 'Drawing of a plane flying', 'plane_6': 'Drawing of a plane flying', 'plane_7': 'Drawing of a plane flying', 'plane_8': 'Drawing of a plane flying', 'plane_9': 'Drawing of a plane flying', 'plane_10': 'Drawing of a plane', 'signal_1': 'Sketch of two sets of traffic lights perpendicular to each other', 'signal_2': 'Sketch of a traffic signal', 'signal_3': 'Drawing of a traffic signal with another signal on its side', 'signal_4': 'Sketch of two sets of traffic lights perpendicular to each other', 'signal_5': 'Drawing of a signal', 'signal_6': 'Drawing of a signal'}\n"
     ]
    }
   ],
   "source": [
    "ground_truth_captions = json.load(open(config[\"ground_truth\"]))\n",
    "print(ground_truth_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffce3ef-a5d1-43d7-bf1e-9b1a2c80f32d",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33420b7b-45f9-4dad-bbd9-e22b203d91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for name, image in sketches:\n",
    "    dataset.append({\n",
    "        \"image\": image,\n",
    "        \"text\": ground_truth_captions[name]\n",
    "    })\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48f83dbd-0af3-4cb1-b5f8-3690ca4df865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d8c211a-d631-49df-9a76-74c240be0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561b286-6f7f-42aa-8a15-a9701c289ac1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ef412b5-ecd5-43af-9446-32d8582049d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# model.to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21627220-6924-470c-b3ba-ffdf310ce467",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(config['epochs'])):\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # input_ids = batch.pop(\"input_ids\").to(DEVICE)\n",
    "        # pixel_values = batch.pop(\"pixel_values\").to(DEVICE)\n",
    "        input_ids = batch.pop(\"input_ids\")\n",
    "        pixel_values = batch.pop(\"pixel_values\")\n",
    "        print(pixel_values.shape)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2383b92e-18b5-471f-8290-294c5d0ff791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "  0%|          | 0/50 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     47\u001b[0m     generated_captions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     decoded_captions \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_decode(generated_captions, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m     all_captions\u001b[38;5;241m.\u001b[39mextend(decoded_captions)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_perplexity(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def lexical_diversity(caption):\n",
    "    tokens = caption.split()\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    return len(set(tokens)) / len(tokens)\n",
    "\n",
    "def n_gram_diversity(caption, n=2):\n",
    "    tokens = caption.split()\n",
    "    if len(tokens) < n:\n",
    "        return 0\n",
    "    n_grams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    return len(set(n_grams)) / len(n_grams)\n",
    "\n",
    "for epoch in tqdm(range(config['epochs'])):\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1}/{config['epochs']}\")\n",
    "    total_loss = 0\n",
    "    all_captions = []\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch.pop(\"input_ids\")\n",
    "        pixel_values = batch.pop(\"pixel_values\")\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate captions for diversity metrics\n",
    "        with torch.no_grad():\n",
    "            generated_captions = model.generate(pixel_values=pixel_values, max_length=20)\n",
    "            decoded_captions = processor.batch_decode(generated_captions, skip_special_tokens=True)\n",
    "            all_captions.extend(decoded_captions)\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Batch {idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Perplexity computation\n",
    "    inputs = {\"input_ids\": input_ids, \"pixel_values\": pixel_values}\n",
    "    perplexity = compute_perplexity(model, inputs)\n",
    "    print(f\"Epoch {epoch + 1} Perplexity: {perplexity:.4f}\")\n",
    "    \n",
    "    # Caption Diversity Metrics\n",
    "    lexical_div = sum(lexical_diversity(caption) for caption in all_captions) / len(all_captions)\n",
    "    bigram_div = sum(n_gram_diversity(caption, n=2) for caption in all_captions) / len(all_captions)\n",
    "    print(f\"Epoch {epoch + 1} Lexical Diversity: {lexical_div:.4f}, Bigram Diversity: {bigram_div:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e88ef-0a67-485a-b553-c337e5115653",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2ca8c160-3e8c-4505-b986-f50897fc50e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Caption...: 100%|██████████| 47/47 [01:36<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike_1': 'isometric view drawing of a police motorcycle', 'bike_2': 'isometric view drawing of a police motorcycle', 'bike_3': 'isometric view drawing of a motorcycle', 'bike_4': 'isometric view drawing of a motorcycle', 'bike_5': 'isometric view drawing of a police motorcycle', 'bike_6': 'isometric view drawing of a motorcycle', 'bike_7': 'isometric view drawing of a police motorcycle', 'bike_8': 'isometric view drawing of a motorcycle', 'bike_9': 'isometric view drawing of a motorcycle', 'car_1': 'isometric view drawing of a car', 'car_2': 'isometric view drawing of a car', 'car_3': 'isometric view drawing of a car', 'car_4': 'sideview drawing of a car', 'car_5': 'sideview drawing of a car', 'car_6': 'sideview drawing of a car', 'car_7': 'isometric view drawing of a car', 'car_8': 'sideview drawing of a car', 'car_9': 'sideview drawing of a car', 'car_10': 'isometric view drawing of a classic car', 'cat_1': 'sketch of a cat', 'cat_2': 'drawing of a cat', 'cat_3': 'drawing of a cat', 'cat_4': 'sideview drawing of a cat', 'cat_5': 'drawing of a cat', 'cat_6': 'drawing of a cat', 'cat_7': 'drawing of a cat', 'cycle_1': 'isometric view drawing of a bicycle', 'cycle_2': 'isometric view drawing of a bicycle', 'cycle_3': 'sideview drawing of a bicycle', 'cycle_4': 'sideview drawing of a bicycle', 'cycle_5': 'isometric view drawing of a bicycle', 'cycle_6': 'isometric view drawing of a bicycle', 'plane_1': 'sideview drawing of a plane', 'plane_2': 'sideview drawing of a plane', 'plane_3': 'sideview drawing of a plane', 'plane_4': 'drawing of a plane flying', 'plane_5': 'drawing of a plane flying', 'plane_6': 'drawing of a plane flying', 'plane_7': 'sideview drawing of a plane', 'plane_8': 'drawing of a plane flying', 'plane_9': 'isometric view drawing of a plane', 'signal_1': 'sketch of two sets of traffic lights', 'signal_2': 'sketch of a traffic signal', 'signal_3': 'sideview drawing of a car', 'signal_4': 'sketch of two sets of traffic lights', 'signal_5': 'drawing of a signal', 'signal_6': 'drawing of a signal'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "for category, sketch in tqdm(sketches, desc=f\"Generating Caption...\"):\n",
    "    inputs = processor(sketch, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    result[category] = caption\n",
    "\n",
    "print(result)\n",
    "\n",
    "with open('img2text_finetuned.json', 'w+') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547919e-925d-44bb-a9bd-ae8747cfa4ba",
   "metadata": {},
   "source": [
    "## Download the model parameters for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f1ca172-d374-422e-996b-a6f33a23a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_blip')\n",
    "processor.save_pretrained('./fine_tuned_blip_processor')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stable Diffusion)",
   "language": "python",
   "name": "stable-diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
